---
phase: 05-results-polish
plan: 04
type: execute
wave: 2
depends_on: ["05-01", "05-02", "05-03"]
files_modified:
  - tests/test_exports.py
  - tests/test_scenarios.py
  - tests/test_visualization.py
  - src/asset_optimization/exports.py
  - src/asset_optimization/scenarios.py
  - src/asset_optimization/visualization.py
  - src/asset_optimization/portfolio.py
  - src/asset_optimization/simulation/result.py
  - src/asset_optimization/optimization/result.py
autonomous: true

must_haves:
  truths:
    - "Export functions have test coverage"
    - "Scenario comparison functions have test coverage"
    - "Visualization functions have test coverage (non-interactive)"
    - "All public API functions have type hints"
  artifacts:
    - path: "tests/test_exports.py"
      provides: "Tests for export functions"
      min_lines: 50
    - path: "tests/test_scenarios.py"
      provides: "Tests for scenario comparison"
      min_lines: 40
    - path: "tests/test_visualization.py"
      provides: "Tests for visualization functions"
      min_lines: 50
  key_links:
    - from: "tests/test_exports.py"
      to: "src/asset_optimization/exports.py"
      via: "pytest imports and tests"
      pattern: "from asset_optimization"
---

<objective>
Add test coverage for Phase 5 modules and ensure type hints across public API

Purpose: Validate exports, scenarios, and visualization modules; ensure DEVX-02 (type hints) compliance
Output: Test files for new modules, type hint completeness
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-results-polish/05-CONTEXT.md

@src/asset_optimization/exports.py
@src/asset_optimization/scenarios.py
@src/asset_optimization/visualization.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test file for exports module</name>
  <files>
    tests/test_exports.py
  </files>
  <action>
Create tests/test_exports.py with comprehensive tests:

```python
"""Tests for export functions."""

import tempfile
from pathlib import Path

import pandas as pd
import pytest

from asset_optimization import (
    SimulationResult,
    SimulationConfig,
    OptimizationResult,
    export_schedule_minimal,
    export_schedule_detailed,
    export_cost_projections,
)


class TestExportScheduleMinimal:
    """Tests for export_schedule_minimal function."""

    def test_exports_correct_columns(self, tmp_path: Path) -> None:
        """Minimal export has exactly asset_id, year, intervention_type, cost."""
        selections = pd.DataFrame({
            'asset_id': ['A1', 'A2'],
            'intervention_type': ['Replace', 'Repair'],
            'cost': [5000.0, 1000.0],
            'risk_score': [0.8, 0.3],
            'rank': [1, 2],
        })
        output_path = tmp_path / 'schedule.parquet'

        export_schedule_minimal(selections, output_path, year=2024)

        result = pd.read_parquet(output_path)
        assert list(result.columns) == ['asset_id', 'year', 'intervention_type', 'cost']

    def test_year_column_added(self, tmp_path: Path) -> None:
        """Year column is added with specified value."""
        selections = pd.DataFrame({
            'asset_id': ['A1'],
            'intervention_type': ['Replace'],
            'cost': [5000.0],
            'risk_score': [0.8],
            'rank': [1],
        })
        output_path = tmp_path / 'schedule.parquet'

        export_schedule_minimal(selections, output_path, year=2030)

        result = pd.read_parquet(output_path)
        assert result['year'].iloc[0] == 2030

    def test_empty_selections(self, tmp_path: Path) -> None:
        """Empty selections produce empty parquet with correct schema."""
        selections = pd.DataFrame({
            'asset_id': [],
            'intervention_type': [],
            'cost': [],
            'risk_score': [],
            'rank': [],
        })
        output_path = tmp_path / 'schedule.parquet'

        export_schedule_minimal(selections, output_path, year=2024)

        result = pd.read_parquet(output_path)
        assert len(result) == 0
        assert list(result.columns) == ['asset_id', 'year', 'intervention_type', 'cost']


class TestExportScheduleDetailed:
    """Tests for export_schedule_detailed function."""

    def test_includes_risk_columns(self, tmp_path: Path) -> None:
        """Detailed export includes risk_before, risk_after, risk_reduction."""
        selections = pd.DataFrame({
            'asset_id': ['A1'],
            'intervention_type': ['Replace'],
            'cost': [5000.0],
            'risk_score': [0.8],
            'rank': [1],
            'risk_before': [0.8],
            'risk_after': [0.0],
        })
        output_path = tmp_path / 'schedule.parquet'

        export_schedule_detailed(selections, output_path, year=2024)

        result = pd.read_parquet(output_path)
        assert 'risk_before' in result.columns
        assert 'risk_after' in result.columns
        assert 'risk_reduction' in result.columns
        assert result['risk_reduction'].iloc[0] == 0.8

    def test_portfolio_join_adds_material(self, tmp_path: Path) -> None:
        """Portfolio join adds material column."""
        selections = pd.DataFrame({
            'asset_id': ['A1', 'A2'],
            'intervention_type': ['Replace', 'Repair'],
            'cost': [5000.0, 1000.0],
            'risk_score': [0.8, 0.3],
            'rank': [1, 2],
        })
        portfolio = pd.DataFrame({
            'asset_id': ['A1', 'A2'],
            'material': ['Cast Iron', 'PVC'],
            'age': [50, 20],
        })
        output_path = tmp_path / 'schedule.parquet'

        export_schedule_detailed(selections, output_path, year=2024, portfolio=portfolio)

        result = pd.read_parquet(output_path)
        assert 'material' in result.columns
        assert 'age' in result.columns
        assert result[result['asset_id'] == 'A1']['material'].iloc[0] == 'Cast Iron'

    def test_asset_type_fallback_to_material(self, tmp_path: Path) -> None:
        """asset_type column is used as material if material not present."""
        selections = pd.DataFrame({
            'asset_id': ['A1'],
            'intervention_type': ['Replace'],
            'cost': [5000.0],
            'risk_score': [0.8],
            'rank': [1],
        })
        portfolio = pd.DataFrame({
            'asset_id': ['A1'],
            'asset_type': ['Water Main'],
        })
        output_path = tmp_path / 'schedule.parquet'

        export_schedule_detailed(selections, output_path, year=2024, portfolio=portfolio)

        result = pd.read_parquet(output_path)
        assert 'material' in result.columns
        assert result['material'].iloc[0] == 'Water Main'


class TestExportCostProjections:
    """Tests for export_cost_projections function."""

    def test_long_format_output(self, tmp_path: Path) -> None:
        """Cost projections are in long format (year, metric, value)."""
        summary = pd.DataFrame({
            'year': [2024, 2025],
            'total_cost': [100000.0, 120000.0],
            'failure_count': [5, 7],
        })
        output_path = tmp_path / 'projections.parquet'

        export_cost_projections(summary, output_path)

        result = pd.read_parquet(output_path)
        assert 'year' in result.columns
        assert 'metric' in result.columns
        assert 'value' in result.columns
        assert set(result['metric'].unique()) == {'total_cost', 'failure_count'}

    def test_multiple_years(self, tmp_path: Path) -> None:
        """Multiple years produce multiple rows per metric."""
        summary = pd.DataFrame({
            'year': [2024, 2025, 2026],
            'total_cost': [100000.0, 120000.0, 110000.0],
            'failure_count': [5, 7, 6],
        })
        output_path = tmp_path / 'projections.parquet'

        export_cost_projections(summary, output_path)

        result = pd.read_parquet(output_path)
        # 3 years * 2 metrics = 6 rows
        assert len(result) == 6


class TestSimulationResultToParquet:
    """Tests for SimulationResult.to_parquet method."""

    @pytest.fixture
    def sim_result(self) -> SimulationResult:
        """Create test SimulationResult."""
        config = SimulationConfig(n_years=3)
        summary = pd.DataFrame({
            'year': [2024, 2025, 2026],
            'total_cost': [100000.0, 120000.0, 110000.0],
            'failure_count': [5, 7, 6],
            'intervention_count': [10, 12, 11],
        })
        return SimulationResult(
            summary=summary,
            cost_breakdown=pd.DataFrame(),
            failure_log=pd.DataFrame({'year': [2024], 'asset_id': ['A1']}),
            config=config,
        )

    def test_summary_format(self, sim_result: SimulationResult, tmp_path: Path) -> None:
        """Summary format exports full summary DataFrame."""
        output_path = tmp_path / 'summary.parquet'
        sim_result.to_parquet(output_path, format='summary')

        result = pd.read_parquet(output_path)
        assert 'year' in result.columns
        assert 'total_cost' in result.columns
        assert len(result) == 3

    def test_cost_projections_format(self, sim_result: SimulationResult, tmp_path: Path) -> None:
        """Cost projections format exports long format."""
        output_path = tmp_path / 'projections.parquet'
        sim_result.to_parquet(output_path, format='cost_projections')

        result = pd.read_parquet(output_path)
        assert 'metric' in result.columns
        assert 'value' in result.columns

    def test_failure_log_format(self, sim_result: SimulationResult, tmp_path: Path) -> None:
        """Failure log format exports failure_log DataFrame."""
        output_path = tmp_path / 'failures.parquet'
        sim_result.to_parquet(output_path, format='failure_log')

        result = pd.read_parquet(output_path)
        assert 'asset_id' in result.columns

    def test_invalid_format_raises(self, sim_result: SimulationResult, tmp_path: Path) -> None:
        """Invalid format raises ValueError."""
        with pytest.raises(ValueError, match="Unknown format"):
            sim_result.to_parquet(tmp_path / 'out.parquet', format='invalid')


class TestOptimizationResultToParquet:
    """Tests for OptimizationResult.to_parquet method."""

    @pytest.fixture
    def opt_result(self) -> OptimizationResult:
        """Create test OptimizationResult."""
        selections = pd.DataFrame({
            'asset_id': ['A1', 'A2'],
            'intervention_type': ['Replace', 'Repair'],
            'cost': [5000.0, 1000.0],
            'risk_score': [0.8, 0.3],
            'rank': [1, 2],
            'risk_before': [0.8, 0.3],
            'risk_after': [0.0, 0.2],
        })
        budget_summary = pd.DataFrame({
            'budget': [10000.0],
            'spent': [6000.0],
            'remaining': [4000.0],
            'utilization_pct': [60.0],
        })
        return OptimizationResult(
            selections=selections,
            budget_summary=budget_summary,
            strategy='greedy',
        )

    def test_minimal_format(self, opt_result: OptimizationResult, tmp_path: Path) -> None:
        """Minimal format has 4 columns."""
        output_path = tmp_path / 'minimal.parquet'
        opt_result.to_parquet(output_path, format='minimal', year=2024)

        result = pd.read_parquet(output_path)
        assert list(result.columns) == ['asset_id', 'year', 'intervention_type', 'cost']

    def test_detailed_format(self, opt_result: OptimizationResult, tmp_path: Path) -> None:
        """Detailed format includes risk columns."""
        output_path = tmp_path / 'detailed.parquet'
        opt_result.to_parquet(output_path, format='detailed', year=2024)

        result = pd.read_parquet(output_path)
        assert 'risk_reduction' in result.columns

    def test_invalid_format_raises(self, opt_result: OptimizationResult, tmp_path: Path) -> None:
        """Invalid format raises ValueError."""
        with pytest.raises(ValueError, match="Unknown format"):
            opt_result.to_parquet(tmp_path / 'out.parquet', format='invalid')
```
  </action>
  <verify>
    cd /Users/henkgriffioen/code/asset-optimization && python -m pytest tests/test_exports.py -v --tb=short
  </verify>
  <done>
    - tests/test_exports.py exists with ~150 lines of tests
    - All export functions have test coverage
    - SimulationResult.to_parquet tested for all formats
    - OptimizationResult.to_parquet tested for minimal and detailed
  </done>
</task>

<task type="auto">
  <name>Task 2: Create test files for scenarios and visualization</name>
  <files>
    tests/test_scenarios.py
    tests/test_visualization.py
  </files>
  <action>
1. Create tests/test_scenarios.py:

```python
"""Tests for scenario comparison functions."""

import pandas as pd
import pytest

from asset_optimization import (
    SimulationResult,
    SimulationConfig,
    compare_scenarios,
    create_do_nothing_baseline,
    compare,
)


class TestCompareScenarios:
    """Tests for compare_scenarios function."""

    @pytest.fixture
    def sample_result(self) -> SimulationResult:
        """Create sample SimulationResult."""
        config = SimulationConfig(n_years=3)
        summary = pd.DataFrame({
            'year': [2024, 2025, 2026],
            'total_cost': [100000.0, 120000.0, 110000.0],
            'failure_count': [5, 7, 6],
            'intervention_count': [10, 12, 11],
        })
        return SimulationResult(
            summary=summary,
            cost_breakdown=pd.DataFrame(),
            failure_log=pd.DataFrame(),
            config=config,
        )

    def test_output_has_correct_columns(self, sample_result: SimulationResult) -> None:
        """Output DataFrame has scenario, year, metric, value columns."""
        result = compare_scenarios({'test': sample_result})

        assert list(result.columns) == ['scenario', 'year', 'metric', 'value']

    def test_multiple_scenarios(self, sample_result: SimulationResult) -> None:
        """Multiple scenarios produce multiple scenario values."""
        result = compare_scenarios({
            'scenario_a': sample_result,
            'scenario_b': sample_result,
        })

        assert set(result['scenario'].unique()) == {'scenario_a', 'scenario_b'}

    def test_custom_metrics(self, sample_result: SimulationResult) -> None:
        """Custom metrics filter to specified columns."""
        result = compare_scenarios(
            {'test': sample_result},
            metrics=['total_cost'],
        )

        assert set(result['metric'].unique()) == {'total_cost'}

    def test_empty_scenarios_returns_empty_df(self) -> None:
        """Empty scenarios dict returns empty DataFrame with correct columns."""
        result = compare_scenarios({})

        assert list(result.columns) == ['scenario', 'year', 'metric', 'value']
        assert len(result) == 0


class TestCreateDoNothingBaseline:
    """Tests for create_do_nothing_baseline function."""

    @pytest.fixture
    def sample_result(self) -> SimulationResult:
        """Create sample SimulationResult."""
        config = SimulationConfig(n_years=3)
        summary = pd.DataFrame({
            'year': [2024, 2025, 2026],
            'total_cost': [100000.0, 120000.0, 110000.0],
            'failure_count': [5, 7, 6],
            'intervention_count': [10, 12, 11],
            'avg_age': [25.0, 26.0, 27.0],
        })
        return SimulationResult(
            summary=summary,
            cost_breakdown=pd.DataFrame(),
            failure_log=pd.DataFrame(),
            config=config,
        )

    def test_returns_simulation_result(self, sample_result: SimulationResult) -> None:
        """Returns a SimulationResult object."""
        baseline = create_do_nothing_baseline(sample_result)

        assert isinstance(baseline, SimulationResult)

    def test_intervention_count_zero(self, sample_result: SimulationResult) -> None:
        """Baseline has zero interventions."""
        baseline = create_do_nothing_baseline(sample_result)

        assert all(baseline.summary['intervention_count'] == 0)

    def test_failures_increase_over_time(self, sample_result: SimulationResult) -> None:
        """Failure count tends to increase over time."""
        baseline = create_do_nothing_baseline(sample_result)

        # Later years should have more failures (general trend)
        first_year = baseline.summary['failure_count'].iloc[0]
        last_year = baseline.summary['failure_count'].iloc[-1]
        assert last_year >= first_year

    def test_config_preserved(self, sample_result: SimulationResult) -> None:
        """Config is preserved from original result."""
        baseline = create_do_nothing_baseline(sample_result)

        assert baseline.config.n_years == sample_result.config.n_years


class TestCompare:
    """Tests for compare convenience function."""

    @pytest.fixture
    def sample_result(self) -> SimulationResult:
        """Create sample SimulationResult."""
        config = SimulationConfig(n_years=3)
        summary = pd.DataFrame({
            'year': [2024, 2025, 2026],
            'total_cost': [100000.0, 120000.0, 110000.0],
            'failure_count': [5, 7, 6],
            'intervention_count': [10, 12, 11],
        })
        return SimulationResult(
            summary=summary,
            cost_breakdown=pd.DataFrame(),
            failure_log=pd.DataFrame(),
            config=config,
        )

    def test_auto_baseline_do_nothing(self, sample_result: SimulationResult) -> None:
        """baseline='do_nothing' auto-generates baseline."""
        result = compare(sample_result, baseline='do_nothing')

        assert 'optimized' in result['scenario'].unique()
        assert 'baseline' in result['scenario'].unique()

    def test_explicit_baseline(self, sample_result: SimulationResult) -> None:
        """Explicit SimulationResult baseline works."""
        result = compare(sample_result, baseline=sample_result)

        assert len(result['scenario'].unique()) == 2

    def test_invalid_baseline_raises(self, sample_result: SimulationResult) -> None:
        """Invalid baseline string raises ValueError."""
        with pytest.raises(ValueError):
            compare(sample_result, baseline='invalid_string')
```

2. Create tests/test_visualization.py:

```python
"""Tests for visualization functions."""

import matplotlib
matplotlib.use('Agg')  # Non-interactive backend

import pandas as pd
import pytest

from asset_optimization import (
    SimulationResult,
    SimulationConfig,
    set_sdk_theme,
    plot_cost_over_time,
    plot_failures_by_year,
    plot_risk_distribution,
    plot_scenario_comparison,
    compare,
)


class TestSetSdkTheme:
    """Tests for set_sdk_theme function."""

    def test_sets_seaborn_style(self) -> None:
        """Theme can be applied without error."""
        # Just verify it runs without exception
        set_sdk_theme()


class TestPlotCostOverTime:
    """Tests for plot_cost_over_time function."""

    @pytest.fixture
    def sample_result(self) -> SimulationResult:
        """Create sample SimulationResult."""
        config = SimulationConfig(n_years=3)
        summary = pd.DataFrame({
            'year': [2024, 2025, 2026],
            'total_cost': [100000.0, 120000.0, 110000.0],
            'failure_count': [5, 7, 6],
        })
        return SimulationResult(
            summary=summary,
            cost_breakdown=pd.DataFrame(),
            failure_log=pd.DataFrame(),
            config=config,
        )

    def test_returns_axes(self, sample_result: SimulationResult) -> None:
        """Function returns matplotlib Axes object."""
        ax = plot_cost_over_time(sample_result)

        assert ax is not None
        import matplotlib.pyplot as plt
        plt.close('all')

    def test_custom_title(self, sample_result: SimulationResult) -> None:
        """Custom title is applied."""
        ax = plot_cost_over_time(sample_result, title='Custom Title')

        assert ax.get_title() == 'Custom Title'
        import matplotlib.pyplot as plt
        plt.close('all')


class TestPlotFailuresByYear:
    """Tests for plot_failures_by_year function."""

    @pytest.fixture
    def sample_result(self) -> SimulationResult:
        """Create sample SimulationResult."""
        config = SimulationConfig(n_years=3)
        summary = pd.DataFrame({
            'year': [2024, 2025, 2026],
            'total_cost': [100000.0, 120000.0, 110000.0],
            'failure_count': [5, 7, 6],
        })
        return SimulationResult(
            summary=summary,
            cost_breakdown=pd.DataFrame(),
            failure_log=pd.DataFrame(),
            config=config,
        )

    def test_returns_axes(self, sample_result: SimulationResult) -> None:
        """Function returns matplotlib Axes object."""
        ax = plot_failures_by_year(sample_result)

        assert ax is not None
        import matplotlib.pyplot as plt
        plt.close('all')


class TestPlotRiskDistribution:
    """Tests for plot_risk_distribution function."""

    def test_returns_axes(self) -> None:
        """Function returns matplotlib Axes object."""
        data = pd.DataFrame({'risk_score': [0.1, 0.3, 0.5, 0.7, 0.9]})
        ax = plot_risk_distribution(data)

        assert ax is not None
        import matplotlib.pyplot as plt
        plt.close('all')

    def test_custom_column_name(self) -> None:
        """Custom risk column name works."""
        data = pd.DataFrame({'failure_probability': [0.1, 0.3, 0.5, 0.7, 0.9]})
        ax = plot_risk_distribution(data, risk_column='failure_probability')

        assert ax is not None
        import matplotlib.pyplot as plt
        plt.close('all')

    def test_missing_column_raises(self) -> None:
        """Missing column raises ValueError."""
        data = pd.DataFrame({'other_column': [0.1, 0.3, 0.5]})

        with pytest.raises(ValueError, match="not found"):
            plot_risk_distribution(data, risk_column='risk_score')


class TestPlotScenarioComparison:
    """Tests for plot_scenario_comparison function."""

    @pytest.fixture
    def comparison_df(self) -> pd.DataFrame:
        """Create sample comparison DataFrame."""
        config = SimulationConfig(n_years=3)
        summary = pd.DataFrame({
            'year': [2024, 2025, 2026],
            'total_cost': [100000.0, 120000.0, 110000.0],
            'failure_count': [5, 7, 6],
        })
        result = SimulationResult(
            summary=summary,
            cost_breakdown=pd.DataFrame(),
            failure_log=pd.DataFrame(),
            config=config,
        )
        return compare(result, baseline='do_nothing')

    def test_returns_axes(self, comparison_df: pd.DataFrame) -> None:
        """Function returns matplotlib Axes object."""
        ax = plot_scenario_comparison(comparison_df, metric='total_cost')

        assert ax is not None
        import matplotlib.pyplot as plt
        plt.close('all')

    def test_invalid_metric_raises(self, comparison_df: pd.DataFrame) -> None:
        """Invalid metric raises ValueError."""
        with pytest.raises(ValueError, match="not found"):
            plot_scenario_comparison(comparison_df, metric='nonexistent_metric')
```
  </action>
  <verify>
    cd /Users/henkgriffioen/code/asset-optimization && python -m pytest tests/test_scenarios.py tests/test_visualization.py -v --tb=short
  </verify>
  <done>
    - tests/test_scenarios.py exists with ~100 lines
    - tests/test_visualization.py exists with ~100 lines
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 3: Type hint audit and run full test suite</name>
  <files>
    src/asset_optimization/exports.py
    src/asset_optimization/scenarios.py
    src/asset_optimization/visualization.py
  </files>
  <action>
1. Verify type hints are present in all new modules. Check each file for:
   - Function parameters have type annotations
   - Return types are specified
   - Union, Optional, List, Dict imported from typing where needed

2. If any type hints are missing, add them. Key functions to verify:
   - exports.py: export_schedule_minimal, export_schedule_detailed, export_cost_projections
   - scenarios.py: compare_scenarios, create_do_nothing_baseline, compare
   - visualization.py: set_sdk_theme, plot_cost_over_time, plot_failures_by_year, plot_risk_distribution, plot_scenario_comparison

3. Run the full test suite to ensure nothing is broken:
   cd /Users/henkgriffioen/code/asset-optimization && python -m pytest tests/ -v --tb=short

4. Verify type hint coverage with a quick check:
   python -c "
   from asset_optimization.exports import export_schedule_minimal
   from asset_optimization.scenarios import compare_scenarios
   from asset_optimization.visualization import plot_cost_over_time
   import inspect

   # Check that functions have annotations
   assert export_schedule_minimal.__annotations__, 'export_schedule_minimal missing annotations'
   assert compare_scenarios.__annotations__, 'compare_scenarios missing annotations'
   assert plot_cost_over_time.__annotations__, 'plot_cost_over_time missing annotations'
   print('Type hints verified')
   "
  </action>
  <verify>
    cd /Users/henkgriffioen/code/asset-optimization && python -m pytest tests/ -v --tb=short 2>&1 | tail -20
  </verify>
  <done>
    - All new modules have type hints on public functions
    - Full test suite passes (including new Phase 5 tests)
    - DEVX-02 (type hints) complete for Phase 5 modules
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_exports.py -v` passes
2. `python -m pytest tests/test_scenarios.py -v` passes
3. `python -m pytest tests/test_visualization.py -v` passes
4. `python -m pytest tests/ -v` full suite passes
5. Type hints present on all public API functions
</verification>

<success_criteria>
- Tests: Export, scenario, and visualization modules have test coverage
- Type hints: All public functions have parameter and return type annotations
- DEVX-02: Type hint requirement satisfied for Phase 5
- No regressions: Full test suite passes
</success_criteria>

<output>
After completion, create `.planning/phases/05-results-polish/05-04-SUMMARY.md`
</output>
