---
phase: 03-simulation-core
plan: 04
type: execute
wave: 3
depends_on: ["03-03"]
files_modified:
  - tests/test_simulation.py
  - tests/conftest.py
autonomous: true

must_haves:
  truths:
    - "Tests verify SimulationConfig validation (n_years > 0, valid failure_response)"
    - "Tests verify SimulationResult convenience methods (total_cost, total_failures)"
    - "Tests verify intervention age effects (Replace->0, Repair->reduced, DoNothing->unchanged)"
    - "Tests verify simulation reproducibility (same seed = same results)"
    - "Tests verify conditional probability calculation"
    - "Tests verify timestep ordering (age, then failures, then interventions)"
    - "Tests verify get_intervention_options returns all intervention types per asset (INTV-04)"
  artifacts:
    - path: "tests/test_simulation.py"
      provides: "Comprehensive simulation test suite"
      min_lines: 200
    - path: "tests/conftest.py"
      provides: "Shared fixtures for simulation tests"
      contains: "def sample_portfolio"
  key_links:
    - from: "tests/test_simulation.py"
      to: "src/asset_optimization/simulation/"
      via: "pytest imports"
      pattern: "from asset_optimization"
---

<objective>
Create comprehensive test suite for the simulation module.

Purpose: Verify all Phase 3 requirements are met with tests for configuration validation, intervention effects, reproducibility, conditional probability, and full simulation workflows.

Output: test_simulation.py with organized test classes covering config, result, interventions, and simulator.
</objective>

<execution_context>
@/Users/henkgriffioen/.claude/get-shit-done/workflows/execute-plan.md
@/Users/henkgriffioen/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-simulation-core/03-CONTEXT.md
@.planning/phases/03-simulation-core/03-03-SUMMARY.md

Testing patterns from Phase 1:
- tests/conftest.py â€” shared fixtures pattern
- Organize tests by class (TestPortfolio, TestValidation, etc.)
- Use pytest parametrize for multiple test cases

Key requirements to test:
- SIMU-01: Multi-timestep simulation
- SIMU-02: Asset state updates (age increments)
- SIMU-03: Intervention effects applied
- SIMU-04: Deterministic with random seed
- SIMU-05: Cumulative metrics tracked
- INTV-01-04: Intervention types, costs, effects
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create simulation test fixtures</name>
  <files>tests/conftest.py</files>
  <action>
Add simulation-specific fixtures to conftest.py:

```python
@pytest.fixture
def sample_portfolio():
    """Create sample portfolio for simulation tests."""
    n_assets = 100
    test_data = pd.DataFrame({
        'asset_id': [f'PIPE-{i:03d}' for i in range(n_assets)],
        'asset_type': ['pipe'] * n_assets,
        'material': ['PVC'] * 50 + ['Cast Iron'] * 50,
        'install_date': pd.date_range('2000-01-01', periods=n_assets, freq='30D'),
        'diameter_mm': [100] * n_assets,
        'length_m': [50.0] * n_assets,
        'condition_score': [80.0] * n_assets,
    })
    return Portfolio.from_dataframe(test_data)

@pytest.fixture
def weibull_model():
    """Create WeibullModel for simulation tests."""
    return WeibullModel({
        'PVC': (2.5, 50.0),
        'Cast Iron': (3.0, 40.0),
    })

@pytest.fixture
def simulation_config():
    """Create default simulation config."""
    return SimulationConfig(n_years=5, random_seed=42)
```

Ensure imports are at top of conftest.py.
  </action>
  <verify>
```bash
cd /Users/henkgriffioen/code/asset-optimization
python -c "
import pytest
import sys
sys.path.insert(0, 'tests')
from conftest import *

# Verify fixtures work
print('Fixtures defined correctly')
"
```
  </verify>
  <done>Simulation fixtures added to conftest.py</done>
</task>

<task type="auto">
  <name>Task 2: Create simulation test suite</name>
  <files>tests/test_simulation.py</files>
  <action>
Create comprehensive test suite with these test classes:

**TestSimulationConfig:**
- test_config_valid_creation
- test_config_immutable (frozen)
- test_config_n_years_validation (must be > 0)
- test_config_failure_response_validation (must be in allowed list)
- test_config_default_values

**TestSimulationResult:**
- test_result_total_cost
- test_result_total_failures
- test_result_repr
- test_result_with_asset_history

**TestInterventionType:**
- test_do_nothing_age_unchanged
- test_replace_age_reset_to_zero
- test_repair_age_reduced
- test_repair_age_clamped_to_zero
- test_custom_intervention
- test_intervention_immutable

**TestSimulator:**
- test_simulator_run_returns_result
- test_simulator_reproducibility_same_seed
- test_simulator_different_seeds_different_results
- test_simulator_ages_increment_each_year
- test_simulator_failures_trigger_intervention
- test_simulator_replace_resets_age
- test_simulator_cumulative_costs
- test_simulator_failure_log_populated

**TestInterventionOptions (INTV-04):**
- test_get_intervention_options_returns_dataframe
- test_get_intervention_options_includes_all_assets
- test_get_intervention_options_includes_all_intervention_types
- test_get_intervention_options_has_required_columns

**TestConditionalProbability:**
- test_conditional_prob_young_assets_low
- test_conditional_prob_old_assets_higher
- test_conditional_prob_handles_zero_survival

Use pytest.mark.parametrize for test variations where appropriate.

Key assertions:
1. Reproducibility: Same config+seed = identical summary DataFrames
2. Age updates: After 1 timestep, ages should be 1 year older (minus interventions)
3. Replace effect: Failed assets with replace response have age=0
4. Cost tracking: summary['total_cost'] should be sum of intervention + failure costs
  </action>
  <verify>
```bash
cd /Users/henkgriffioen/code/asset-optimization
pytest tests/test_simulation.py -v --tb=short
```
  </verify>
  <done>Comprehensive simulation test suite passes</done>
</task>

</tasks>

<verification>
After both tasks complete:

```bash
cd /Users/henkgriffioen/code/asset-optimization

# Run all tests
pytest tests/ -v --tb=short

# Verify coverage of simulation module
pytest tests/test_simulation.py -v --tb=short -x

# Check test count
pytest tests/test_simulation.py --collect-only | tail -5
```
</verification>

<success_criteria>
- All tests pass (pytest exit code 0)
- TestSimulationConfig: 5+ tests for config validation
- TestSimulationResult: 4+ tests for result methods
- TestInterventionType: 6+ tests for intervention effects
- TestSimulator: 8+ tests for simulation behavior
- TestInterventionOptions: 4+ tests for get_intervention_options (INTV-04)
- TestConditionalProbability: 3+ tests for probability calculations
- Fixtures in conftest.py are reusable
</success_criteria>

<output>
After completion, create `.planning/phases/03-simulation-core/03-04-SUMMARY.md`
</output>
